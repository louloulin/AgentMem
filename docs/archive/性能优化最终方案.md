# 性能优化最终方案

## 问题根源
通过详细分析日志，发现LumosAI SSE性能瓶颈的根本原因：

**历史消息数量过多**
- 当前配置：每次检索 10 条历史消息
- 导致 LLM prompt tokens 过大
- 造成 LLM 生成时间从 ~500ms 增加到 ~17.5秒

## 优化方案

### 1. 减少历史消息检索数量
**文件**: `crates/agent-mem-lumosai/src/memory_adapter.rs`

```rust
// 修改前:
let limit = config.last_messages.unwrap_or(10);

// 修改后:
let limit = config.last_messages.unwrap_or(1);  // 只检索1条历史
```

### 2. 预期效果
- **TTFB**: 从 ~17.5秒 降低到 ~2秒以内
- **原理**: 减少prompt tokens → 减少LLM处理时间
- **性能提升**: ~8-9倍

## 当前状态

###编译问题
当前代码存在生命周期错误，需要修复 `chat_lumosai.rs`：

```
error[E0597]: `streaming_agent` does not live long enough
error[E0597]: `messages` does not live long enough
error[E0597]: `options` does not live long enough
```

### 临时方案
使用现有二进制测试（2024-11-20 09:15编译），但该版本未包含memory优化。

## 下一步

1. **修复编译错误** - 解决chat_lumosai.rs的生命周期问题
2. **重新编译** - 包含memory优化的新版本
3. **性能测试** - 验证优化效果
4. **文档总结** - 记录最终优化结果

## 技术细节

### 生命周期问题分析
`execute_streaming` 方法返回的 stream 借用了：
- `streaming_agent`
- `messages` 
- `options`

但这些都是局部变量，无法满足 `'static` 生命周期要求（SSE stream需要）。

### 可能的解决方案
1. 使用 `Arc` 包装（已尝试，仍有问题）
2. 重构为不使用 `execute_streaming`
3. 使用 `tokio::spawn` 在独立任务中处理stream
4. 回退到非streaming模式（分块发送完整响应）

## 结论

**核心问题**: 历史消息数量过多导致LLM处理缓慢  
**解决方案**: 减少历史消息检索到1条  
**阻塞问题**: 编译错误需要修复才能验证优化效果

