#!/usr/bin/env python3
"""
AgentMem Python SDK - RAG é—®ç­”ç³»ç»Ÿç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ AgentMem æ„å»º RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) ç³»ç»Ÿï¼š
- æ–‡æ¡£ç´¢å¼•
- è¯­ä¹‰æ£€ç´¢
- ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆ
- ç­”æ¡ˆç”Ÿæˆ

è¿è¡Œæ–¹å¼:
```bash
export AGENTMEM_API_BASE_URL=http://localhost:8080
export AGENTMEM_API_KEY=your_api_key
export OPENAI_API_KEY=sk-...  # å¦‚æœä½¿ç”¨ LLM

python rag_qa.py
```

é¢„æœŸè¾“å‡º:
```
ğŸ“š AgentMem RAG é—®ç­”ç³»ç»Ÿç¤ºä¾‹

âœ… åˆå§‹åŒ–å®Œæˆ

ğŸ“– æ­¥éª¤ 1: ç´¢å¼•æ–‡æ¡£
   âœ… ç´¢å¼•: "Rust æ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€..."
   âœ… ç´¢å¼•: "Python æ˜¯ä¸€é—¨é«˜çº§ç¼–ç¨‹è¯­è¨€..."
   âœ… ç´¢å¼•: "JavaScript æ˜¯ä¸€é—¨è„šæœ¬è¯­è¨€..."
   âœ… å·²ç´¢å¼• 3 ä»½æ–‡æ¡£

ğŸ” æ­¥éª¤ 2: è¯­ä¹‰æ£€ç´¢
   é—®é¢˜: "Rust æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
   âœ… æ£€ç´¢åˆ° 2 ä¸ªç›¸å…³ç‰‡æ®µ:
      1. Rust æ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œæ³¨é‡å®‰å…¨ã€å¹¶å‘å’Œæ€§èƒ½ (0.95)
      2. Rust çš„æ‰€æœ‰æƒç³»ç»Ÿç¡®ä¿å†…å­˜å®‰å…¨ (0.89)

ğŸ’¡ æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ
   âœ… ç”Ÿæˆç­”æ¡ˆ:
      æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ŒRust æœ‰ä»¥ä¸‹ç‰¹ç‚¹:
      1. ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œæ³¨é‡å®‰å…¨ã€å¹¶å‘å’Œæ€§èƒ½
      2. æ‰€æœ‰æƒç³»ç»Ÿç¡®ä¿å†…å­˜å®‰å…¨

ğŸ¯ æ­¥éª¤ 4: å¤šè½®é—®ç­”
   Q: "Python é€‚åˆåšä»€ä¹ˆï¼Ÿ"
   A: Python é€‚åˆæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€Web å¼€å‘...

   Q: "å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
   A: Python çš„ä¼˜åŠ¿æ˜¯ç®€æ´æ˜“è¯»ã€ç”Ÿæ€ä¸°å¯Œ...

ğŸ‰ å®Œæˆï¼
```
"""

import asyncio
import os
from typing import List, Dict, Optional
from dataclasses import dataclass

try:
    from agentmem import AgentMemClient, Config, SearchQuery, MemoryType
except ImportError:
    print("âš ï¸  AgentMem SDK æœªå®‰è£…")
    print("   å®‰è£…æ–¹å¼: pip install agentmem")
    exit(1)


@dataclass
class Document:
    """æ–‡æ¡£"""
    title: str
    content: str
    source: str = ""


class RAGSystem:
    """RAG é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, client: AgentMemClient, user_id: str):
        """åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
        self.client = client
        self.user_id = user_id
        self.agent_id = "rag_system"

    async def index_document(self, doc: Document) -> str:
        """ç´¢å¼•æ–‡æ¡£"""
        # å°†æ–‡æ¡£å†…å®¹åˆ†æ®µå­˜å‚¨
        memory_id = await self.client.add_memory(
            content=f"{doc.title}: {doc.content}",
            agent_id=self.agent_id,
            user_id=self.user_id,
            memory_type=MemoryType.SEMANTIC,
            metadata={
                "type": "document",
                "title": doc.title,
                "source": doc.source,
                "indexed_at": str(asyncio.get_event_loop().time()),
            },
        )
        return memory_id

    async def batch_index(self, documents: List[Document]) -> List[str]:
        """æ‰¹é‡ç´¢å¼•æ–‡æ¡£"""
        memory_ids = []
        for doc in documents:
            memory_id = await self.index_document(doc)
            memory_ids.append(memory_id)
        return memory_ids

    async def retrieve(self, query: str, top_k: int = 3) -> List[dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        search_query = SearchQuery(
            query=query,
            user_id=self.user_id,
            limit=top_k,
            threshold=0.7,
        )
        results = await self.client.search_memories(search_query)
        return results

    async def generate_answer_simple(self, query: str, context: List[dict]) -> str:
        """ç”Ÿæˆç®€å•ç­”æ¡ˆï¼ˆä¸ä½¿ç”¨ LLMï¼‰"""
        if not context:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æå–ä¸Šä¸‹æ–‡å†…å®¹
        contexts = [c.get("content", "") for c in context]

        # æ„å»ºç­”æ¡ˆ
        answer = f"æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œ{query}\n\n"

        for i, ctx in enumerate(contexts, 1):
            answer += f"{i}. {ctx}\n"

        return answer

    async def generate_answer_llm(self, query: str, context: List[dict]) -> str:
        """ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆéœ€è¦ OpenAI APIï¼‰"""
        if not context:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # è¿™é‡Œå¯ä»¥é›†æˆ OpenAI API
        # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦å®‰è£… openai åº“ï¼‰:
        #
        # import openai
        # openai.api_key = os.getenv("OPENAI_API_KEY")
        #
        # context_text = "\n".join([
        #     f"- {c.get('content', '')}"
        #     for c in context
        # ])
        #
        # prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯´"æŠ±æ­‰ï¼Œæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯"ã€‚
        #
        # æ–‡æ¡£ç‰‡æ®µ:
        # {context_text}
        #
        # é—®é¢˜: {query}
        #
        # å›ç­”:"""
        #
        # response = await openai.ChatCompletion.acreate(
        #     model="gpt-4",
        #     messages=[
        #         {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„é—®ç­”åŠ©æ‰‹ã€‚"},
        #         {"role": "user", "content": prompt}
        #     ]
        # )
        #
        # return response.choices[0].message.content

        # ç®€åŒ–ç‰ˆæœ¬
        return await self.generate_answer_simple(query, context)

    async def ask(self, query: str, use_llm: bool = False) -> str:
        """æé—®"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = await self.retrieve(query)

        # ç”Ÿæˆç­”æ¡ˆ
        if use_llm:
            answer = await self.generate_answer_llm(query, context)
        else:
            answer = await self.generate_answer_simple(query, context)

        return answer


async def demo_document_indexing(rag: RAGSystem):
    """æ¼”ç¤ºæ–‡æ¡£ç´¢å¼•"""
    print("\nğŸ“– æ­¥éª¤ 1: ç´¢å¼•æ–‡æ¡£")
    print("---")

    documents = [
        Document(
            title="Rust ç¼–ç¨‹è¯­è¨€",
            content="Rust æ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œæ³¨é‡å®‰å…¨ã€å¹¶å‘å’Œæ€§èƒ½ã€‚Rust çš„æ‰€æœ‰æƒç³»ç»Ÿç¡®ä¿å†…å­˜å®‰å…¨ï¼Œæ— éœ€åƒåœ¾å›æ”¶ã€‚",
            source="rust_doc.md"
        ),
        Document(
            title="Python ç¼–ç¨‹è¯­è¨€",
            content="Python æ˜¯ä¸€é—¨é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è‘—ç§°ã€‚Python å¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€Web å¼€å‘ç­‰é¢†åŸŸã€‚",
            source="python_doc.md"
        ),
        Document(
            title="JavaScript ç¼–ç¨‹è¯­è¨€",
            content="JavaScript æ˜¯ä¸€é—¨è„šæœ¬è¯­è¨€ï¼Œä¸»è¦ç”¨äº Web å¼€å‘ã€‚éšç€ Node.js çš„å‡ºç°ï¼ŒJavaScript ä¹Ÿå¯ä»¥ç”¨äºæœåŠ¡ç«¯å¼€å‘ã€‚",
            source="js_doc.md"
        ),
        Document(
            title="Go ç¼–ç¨‹è¯­è¨€",
            content="Go æ˜¯ä¸€é—¨å¼€æºç¼–ç¨‹è¯­è¨€ï¼Œä¸“ä¸ºæ„å»ºç®€å•ã€å¯é å’Œé«˜æ•ˆçš„è½¯ä»¶è€Œè®¾è®¡ã€‚Go ç‰¹åˆ«é€‚åˆå¹¶å‘ç¼–ç¨‹å’Œç½‘ç»œæœåŠ¡ã€‚",
            source="go_doc.md"
        ),
    ]

    memory_ids = await rag.batch_index(documents)

    for doc, memory_id in zip(documents, memory_ids):
        print(f"   âœ… ç´¢å¼•: \"{doc.title}\" -> {memory_id}")

    print(f"\n   âœ… å·²ç´¢å¼• {len(documents)} ä»½æ–‡æ¡£")


async def demo_semantic_retrieval(rag: RAGSystem):
    """æ¼”ç¤ºè¯­ä¹‰æ£€ç´¢"""
    print("\nğŸ” æ­¥éª¤ 2: è¯­ä¹‰æ£€ç´¢")
    print("---")

    queries = [
        "Rust æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "Python é€‚åˆåšä»€ä¹ˆï¼Ÿ",
        "JavaScript æœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿ",
    ]

    for query in queries:
        print(f"\n   é—®é¢˜: \"{query}\"")

        results = await rag.retrieve(query, top_k=2)

        print(f"   âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ:")

        for i, result in enumerate(results, 1):
            content = result.get("content", "")
            score = result.get("score", 0.0)
            print(f"      {i}. {content} ({score:.2f})")


async def demo_answer_generation(rag: RAGSystem):
    """æ¼”ç¤ºç­”æ¡ˆç”Ÿæˆ"""
    print("\nğŸ’¡ æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ")
    print("---")

    query = "Rust æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    print(f"   é—®é¢˜: \"{query}\"")

    # æ£€ç´¢ä¸Šä¸‹æ–‡
    context = await rag.retrieve(query, top_k=2)

    print(f"\n   æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:")
    for i, ctx in enumerate(context, 1):
        content = ctx.get("content", "")
        print(f"      {i}. {content}")

    # ç”Ÿæˆç­”æ¡ˆ
    answer = await rag.generate_answer_simple(query, context)

    print(f"\n   âœ… ç”Ÿæˆç­”æ¡ˆ:")
    print(f"   {answer}")


async def demo_multi_turn_qa(rag: RAGSystem):
    """æ¼”ç¤ºå¤šè½®é—®ç­”"""
    print("\nğŸ¯ æ­¥éª¤ 4: å¤šè½®é—®ç­”")
    print("---")

    conversations = [
        ("Python é€‚åˆåšä»€ä¹ˆï¼Ÿ", "Python é€‚åˆæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€Web å¼€å‘ç­‰é¢†åŸŸ"),
        ("å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ", "Python çš„ä¼˜åŠ¿æ˜¯ç®€æ´æ˜“è¯»ã€ç”Ÿæ€ä¸°å¯Œã€æ˜“äºå­¦ä¹ "),
        ("æœ‰ä»€ä¹ˆæµè¡Œçš„æ¡†æ¶ï¼Ÿ", "æµè¡Œçš„ Python æ¡†æ¶åŒ…æ‹¬ Djangoã€Flaskã€FastAPI ç­‰"),
    ]

    for question, expected_answer in conversations:
        print(f"\n   Q: \"{question}\"")

        answer = await rag.ask(question)

        # æ˜¾ç¤ºå‰ä¸¤è¡Œ
        answer_lines = answer.split("\n")[:2]
        short_answer = "\n   ".join(answer_lines)

        print(f"   A: {short_answer}...")


async def demo_domain_specific_qa(rag: RAGSystem):
    """æ¼”ç¤ºé¢†åŸŸç‰¹å®šé—®ç­”"""
    print("\nğŸ“ æ­¥éª¤ 5: é¢†åŸŸç‰¹å®šé—®ç­”")
    print("---")

    # æ·»åŠ æŠ€æœ¯æ–‡æ¡£
    tech_docs = [
        Document(
            title="Rust æ‰€æœ‰æƒç³»ç»Ÿ",
            content="Rust çš„æ‰€æœ‰æƒç³»ç»Ÿæ˜¯å…¶æœ€ç‹¬ç‰¹çš„ç‰¹æ€§ã€‚æ¯ä¸ªå€¼éƒ½æœ‰ä¸€ä¸ªæ‰€æœ‰è€…ï¼Œä¸”åŒä¸€æ—¶é—´åªèƒ½æœ‰ä¸€ä¸ªæ‰€æœ‰è€…ã€‚å½“æ‰€æœ‰è€…è¶…å‡ºä½œç”¨åŸŸï¼Œå€¼å°†è¢«ä¸¢å¼ƒã€‚",
            source="ownership.md"
        ),
        Document(
            title="Rust å€Ÿç”¨æ£€æŸ¥",
            content="Rust çš„å€Ÿç”¨æ£€æŸ¥å™¨ç¡®ä¿å¼•ç”¨æ€»æ˜¯æœ‰æ•ˆçš„ã€‚ä½ å¯ä»¥æ‹¥æœ‰ä¸å¯å˜å¼•ç”¨ï¼ˆ&Tï¼‰æˆ–å¯å˜å¼•ç”¨ï¼ˆ&mut Tï¼‰ï¼Œä½†ä¸èƒ½åŒæ—¶æ‹¥æœ‰ä¸¤è€…ã€‚",
            source="borrowing.md"
        ),
    ]

    await rag.batch_index(tech_docs)
    print("   âœ… å·²æ·»åŠ æŠ€æœ¯æ–‡æ¡£")

    # æŠ€æœ¯é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯ Rust çš„æ‰€æœ‰æƒï¼Ÿ",
        "Rust å¦‚ä½•ä¿è¯å†…å­˜å®‰å…¨ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å€Ÿç”¨æ£€æŸ¥ï¼Ÿ",
    ]

    for question in questions:
        print(f"\n   é—®é¢˜: \"{question}\"")

        answer = await rag.ask(question)

        # æ˜¾ç¤ºç¬¬ä¸€è¡Œ
        first_line = answer.split("\n")[0]
        print(f"   å›ç­”: {first_line}...")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“š AgentMem RAG é—®ç­”ç³»ç»Ÿç¤ºä¾‹\n")
    print("è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†:")
    print("  1. æ–‡æ¡£ç´¢å¼•")
    print("  2. è¯­ä¹‰æ£€ç´¢")
    print("  3. ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆ")
    print("  4. å¤šè½®é—®ç­”")
    print("  5. é¢†åŸŸç‰¹å®šçŸ¥è¯†")
    print()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    api_base_url = os.getenv("AGENTMEM_API_BASE_URL", "http://localhost:8080")
    api_key = os.getenv("AGENTMEM_API_KEY", "demo_key")

    config = Config(
        api_base_url=api_base_url,
        api_key=api_key,
    )

    async with AgentMemClient(config) as client:
        print("âœ… åˆå§‹åŒ–å®Œæˆ")

        # åˆ›å»º RAG ç³»ç»Ÿ
        rag = RAGSystem(
            client=client,
            user_id="rag_user",
        )

        # æ¼”ç¤ºå„ç§åŠŸèƒ½
        await demo_document_indexing(rag)
        await demo_semantic_retrieval(rag)
        await demo_answer_generation(rag)
        await demo_multi_turn_qa(rag)
        await demo_domain_specific_qa(rag)

        # æ˜¾ç¤ºç»Ÿè®¡
        all_memories = await client.get_all_memories(
            user_id="rag_user",
            limit=100,
        )

        print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
        print(f"   å·²ç´¢å¼•æ–‡æ¡£: {len(all_memories)}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        exit(1)


# ============================================
# é«˜çº§åŠŸèƒ½: æ–‡æ¡£åˆ†å—
# ============================================
#
# å¯¹äºé•¿æ–‡æ¡£ï¼Œéœ€è¦å…ˆåˆ†å—å†ç´¢å¼•:
#
# ```python
# def chunk_document(text: str, chunk_size: int = 500) -> List[str]:
#     """å°†æ–‡æ¡£åˆ†æˆå—"""
#     chunks = []
#     sentences = text.split("ã€‚")
#
#     current_chunk = ""
#     for sentence in sentences:
#         if len(current_chunk) + len(sentence) < chunk_size:
#             current_chunk += sentence + "ã€‚"
#         else:
#             chunks.append(current_chunk.strip())
#             current_chunk = sentence + "ã€‚"
#
#     if current_chunk:
#         chunks.append(current_chunk.strip())
#
#     return chunks
#
# async def index_long_document(rag: RAGSystem, doc: Document):
#     """ç´¢å¼•é•¿æ–‡æ¡£"""
#     chunks = chunk_document(doc.content)
#
#     for i, chunk in enumerate(chunks):
#         chunk_doc = Document(
#             title=f"{doc.title} (éƒ¨åˆ† {i+1}/{len(chunks)})",
#             content=chunk,
#             source=doc.source,
#         )
#         await rag.index_document(chunk_doc)
#
#     print(f"âœ… æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå—å¹¶ç´¢å¼•")
# ```
#
# ============================================
# é«˜çº§åŠŸèƒ½: æ··åˆæ£€ç´¢
# ============================================
#
# ç»“åˆè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢:
#
# ```python
# async def hybrid_retrieve(rag: RAGSystem, query: str) -> List[dict]:
#     """æ··åˆæ£€ç´¢"""
#     # è¯­ä¹‰æ£€ç´¢
#     semantic_results = await rag.retrieve(query, top_k=5)
#
#     # å…³é”®è¯æ£€ç´¢ï¼ˆå‡è®¾æœ‰è¿™ä¸ªåŠŸèƒ½ï¼‰
#     # keyword_results = await rag.keyword_search(query, top_k=5)
#
#     # åˆå¹¶å’Œé‡æ–°æ’åº
#     # è¿™é‡Œç®€åŒ–å¤„ç†
#     return semantic_results
# ```
#
# ============================================
# é«˜çº§åŠŸèƒ½: ç­”æ¡ˆè´¨é‡è¯„ä¼°
# ============================================
#
# ```python
# def evaluate_answer(answer: str, context: List[dict]) -> float:
#     """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
#     # ç®€å•çš„è¯„ä¼°æŒ‡æ ‡
#     score = 0.0
#
#     # 1. ç­”æ¡ˆé•¿åº¦
#     if len(answer) > 50:
#         score += 0.3
#
#     # 2. æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
#     context_text = " ".join([c.get("content", "") for c in context])
#     words_in_context = sum(1 for word in answer.split() if word in context_text)
#     if words_in_context > 0:
#         score += 0.4 * (words_in_context / len(answer.split()))
#
#     # 3. ç­”æ¡ˆå®Œæ•´æ€§
#     if "ã€‚" in answer or "." in answer:
#         score += 0.3
#
#     return min(score, 1.0)
# ```
