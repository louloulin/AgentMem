#!/usr/bin/env python3
"""
AgentMem Python SDK - å¤šæ¨¡æ€æœç´¢ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ AgentMem å¤„ç†å’Œæœç´¢å¤šç§ç±»å‹çš„æ•°æ®ï¼š
- å›¾åƒæè¿°æœç´¢
- éŸ³é¢‘è½¬å½•æœç´¢
- æ–‡æ¡£æœç´¢
- è·¨æ¨¡æ€æ£€ç´¢

è¿è¡Œæ–¹å¼:
```bash
export AGENTMEM_API_BASE_URL=http://localhost:8080
export AGENTMEM_API_KEY=your_api_key

python multimodal_search.py
```

é¢„æœŸè¾“å‡º:
```
ğŸ¨ AgentMem å¤šæ¨¡æ€æœç´¢ç¤ºä¾‹

âœ… åˆå§‹åŒ–å®Œæˆ

ğŸ“¸ æ­¥éª¤ 1: å›¾åƒæè¿°ç´¢å¼•
   âœ… ç´¢å¼•: "æ—¥è½æµ·æ»©ç…§ç‰‡ï¼Œæ©™è‰²å¤©ç©º"
   âœ… ç´¢å¼•: "åŸå¸‚å¤œæ™¯ï¼Œç¯å…‰ç’€ç’¨"
   âœ… ç´¢å¼•: "çŒ«åœ¨é˜³å…‰ä¸‹ç¡è§‰"

ğŸµ æ­¥éª¤ 2: éŸ³é¢‘è½¬å½•ç´¢å¼•
   âœ… ç´¢å¼•: "ä¼šè®®è®¨è®ºäº†é¡¹ç›®è¿›åº¦"
   âœ… ç´¢å¼•: "ç”µè¯ç•™è¨€: æ˜å¤©å¼€ä¼š"
   âœ… ç´¢å¼•: "æ’­å®¢: AI çš„æœªæ¥"

ğŸ“„ æ­¥éª¤ 3: æ–‡æ¡£ç´¢å¼•
   âœ… ç´¢å¼•: "é¡¹ç›®æŠ¥å‘Š..."
   âœ… ç´¢å¼•: "ä¼šè®®çºªè¦..."

ğŸ” æ­¥éª¤ 4: è·¨æ¨¡æ€æœç´¢
   æœç´¢: "ä¼šè®®"
   âœ… æ‰¾åˆ° 3 æ¡ç»“æœ:
      1. [éŸ³é¢‘] ä¼šè®®è®¨è®ºäº†é¡¹ç›®è¿›åº¦
      2. [éŸ³é¢‘] ç”µè¯ç•™è¨€: æ˜å¤©å¼€ä¼š
      3. [æ–‡æ¡£] ä¼šè®®çºªè¦

ğŸ¯ æ­¥éª¤ 5: æ¨¡æ€è¿‡æ»¤
   æœç´¢: "ç…§ç‰‡" + è¿‡æ»¤: type=image
   âœ… æ‰¾åˆ° 3 æ¡å›¾åƒç»“æœ

ğŸ‰ å®Œæˆï¼
```
"""

import asyncio
import os
from typing import List, Dict, Optional
from enum import Enum
from dataclasses import dataclass


try:
    from agentmem import AgentMemClient, Config, SearchQuery, MemoryType
except ImportError:
    print("âš ï¸  AgentMem SDK æœªå®‰è£…")
    print("   å®‰è£…æ–¹å¼: pip install agentmem")
    exit(1)


class ContentType(Enum):
    """å†…å®¹ç±»å‹"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    DOCUMENT = "document"


@dataclass
class MultimodalContent:
    """å¤šæ¨¡æ€å†…å®¹"""
    content: str
    content_type: ContentType
    source: str = ""
    metadata: Dict[str, str] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class MultimodalSearchEngine:
    """å¤šæ¨¡æ€æœç´¢å¼•æ“"""

    def __init__(self, client: AgentMemClient, user_id: str):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        self.client = client
        self.user_id = user_id
        self.agent_id = "multimodal_search"

    async def index_content(self, item: MultimodalContent) -> str:
        """ç´¢å¼•å†…å®¹"""
        metadata = {
            "type": item.content_type.value,
            "source": item.source,
            **item.metadata,
        }

        memory_id = await self.client.add_memory(
            content=item.content,
            agent_id=self.agent_id,
            user_id=self.user_id,
            memory_type=MemoryType.SEMANTIC,
            metadata=metadata,
        )

        return memory_id

    async def batch_index(self, items: List[MultimodalContent]) -> List[str]:
        """æ‰¹é‡ç´¢å¼•"""
        memory_ids = []
        for item in items:
            memory_id = await self.index_content(item)
            memory_ids.append(memory_id)
        return memory_ids

    async def search(
        self,
        query: str,
        content_type: Optional[ContentType] = None,
        top_k: int = 5,
        threshold: float = 0.7
    ) -> List[dict]:
        """æœç´¢å†…å®¹"""
        search_query = SearchQuery(
            query=query,
            user_id=self.user_id,
            limit=top_k,
            threshold=threshold,
        )

        results = await self.client.search_memories(search_query)

        # å¦‚æœæŒ‡å®šäº†å†…å®¹ç±»å‹ï¼Œè¿›è¡Œè¿‡æ»¤
        if content_type:
            results = [
                r for r in results
                if r.get("metadata", {}).get("type") == content_type.value
            ]

        return results

    async def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        all_memories = await self.client.get_all_memories(
            user_id=self.user_id,
            limit=1000,
        )

        stats = {}
        for memory in all_memories:
            content_type = memory.get("metadata", {}).get("type", "unknown")
            stats[content_type] = stats.get(content_type, 0) + 1

        return stats


async def demo_image_indexing(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºå›¾åƒç´¢å¼•"""
    print("\nğŸ“¸ æ­¥éª¤ 1: å›¾åƒæè¿°ç´¢å¼•")
    print("---")

    images = [
        MultimodalContent(
            content="æ—¥è½æµ·æ»©ç…§ç‰‡ï¼Œæœ‰æ©™è‰²çš„å¤©ç©ºå’Œè“è‰²çš„å¤§æµ·",
            content_type=ContentType.IMAGE,
            source="image_001.jpg",
            metadata={"time": "2025-01-01 18:00", "location": "æµ·æ»©"},
        ),
        MultimodalContent(
            content="åŸå¸‚çš„å¤œæ™¯ï¼Œç¯å…‰ç’€ç’¨ï¼Œé«˜æ¥¼æ—ç«‹",
            content_type=ContentType.IMAGE,
            source="image_002.jpg",
            metadata={"time": "2025-01-02 20:00", "location": "å¸‚ä¸­å¿ƒ"},
        ),
        MultimodalContent(
            content="ä¸€åªæ©˜è‰²çš„çŒ«åœ¨é˜³å…‰ä¸‹ç¡è§‰ï¼Œå§¿æ€å¯çˆ±",
            content_type=ContentType.IMAGE,
            source="image_003.jpg",
            metadata={"time": "2025-01-03 14:00", "location": "å®¶é‡Œ"},
        ),
    ]

    memory_ids = await engine.batch_index(images)

    for img, memory_id in zip(images, memory_ids):
        print(f"   âœ… ç´¢å¼•: \"{img.content[:30]}...\" -> {memory_id}")


async def demo_audio_indexing(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºéŸ³é¢‘ç´¢å¼•"""
    print("\nğŸµ æ­¥éª¤ 2: éŸ³é¢‘è½¬å½•ç´¢å¼•")
    print("---")

    audio_files = [
        MultimodalContent(
            content="ä¼šè®®è®¨è®ºäº† Q4 çš„é¡¹ç›®è¿›åº¦ï¼Œç¡®å®šäº†ä¸‹ä¸€é˜¶æ®µçš„ç›®æ ‡",
            content_type=ContentType.AUDIO,
            source="meeting_001.mp3",
            metadata={"duration": "15:30", "speaker": "é¡¹ç›®ç»ç†"},
        ),
        MultimodalContent(
            content="ç”µè¯ç•™è¨€: æ˜å¤©ä¸‹åˆä¸‰ç‚¹å¼€ä¼šï¼Œè¯·å‡†æ—¶å‚åŠ ",
            content_type=ContentType.AUDIO,
            source="voicemail_001.mp3",
            metadata={"duration": "0:45", "caller": "å¼ ä¸‰"},
        ),
        MultimodalContent(
            content="æ’­å®¢æ‘˜è¦: è®¨è®ºäº† AI æŠ€æœ¯çš„æœªæ¥å‘å±•è¶‹åŠ¿å’Œåº”ç”¨å‰æ™¯",
            content_type=ContentType.AUDIO,
            source="podcast_001.mp3",
            metadata={"duration": "45:00", "host": "ç§‘æŠ€ä¸»æ’­"},
        ),
    ]

    memory_ids = await engine.batch_index(audio_files)

    for audio, memory_id in zip(audio_files, memory_ids):
        print(f"   âœ… ç´¢å¼•: \"{audio.content[:30]}...\" -> {memory_id}")


async def demo_document_indexing(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºæ–‡æ¡£ç´¢å¼•"""
    print("\nğŸ“„ æ­¥éª¤ 3: æ–‡æ¡£ç´¢å¼•")
    print("---")

    documents = [
        MultimodalContent(
            content="é¡¹ç›®æŠ¥å‘Š: æœ¬å­£åº¦å®Œæˆäº†æ ¸å¿ƒåŠŸèƒ½å¼€å‘ï¼Œæµ‹è¯•è¦†ç›–ç‡è¾¾åˆ° 85%",
            content_type=ContentType.DOCUMENT,
            source="report_q4.pdf",
            metadata={"pages": "15", "author": "é¡¹ç›®ç»ç†"},
        ),
        MultimodalContent(
            content="ä¼šè®®çºªè¦: è®¨è®ºäº†æ–°åŠŸèƒ½çš„è®¾è®¡æ–¹æ¡ˆå’Œå®ç°è®¡åˆ’",
            content_type=ContentType.DOCUMENT,
            source="minutes_001.docx",
            metadata={"pages": "3", "date": "2025-01-01"},
        ),
        MultimodalContent(
            content="æŠ€æœ¯æ–‡æ¡£: Rust è¯­è¨€çš„å¹¶å‘ç¼–ç¨‹æ¨¡å‹å’Œæœ€ä½³å®è·µ",
            content_type=ContentType.DOCUMENT,
            source="rust_concurrency.md",
            metadata={"pages": "20", "author": "æŠ€æœ¯å›¢é˜Ÿ"},
        ),
    ]

    memory_ids = await engine.batch_index(documents)

    for doc, memory_id in zip(documents, memory_ids):
        print(f"   âœ… ç´¢å¼•: \"{doc.content[:30]}...\" -> {memory_id}")


async def demo_cross_modal_search(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºè·¨æ¨¡æ€æœç´¢"""
    print("\nğŸ” æ­¥éª¤ 4: è·¨æ¨¡æ€æœç´¢")
    print("---")

    searches = [
        ("ä¼šè®®", "æœç´¢ä¼šè®®ç›¸å…³å†…å®¹"),
        ("é¡¹ç›®", "æœç´¢é¡¹ç›®ç›¸å…³å†…å®¹"),
        ("å¤œæ™¯", "æœç´¢å¤œæ™¯å›¾ç‰‡"),
    ]

    for query, description in searches:
        print(f"\n   æœç´¢: \"{query}\" ({description})")

        results = await engine.search(query, top_k=3)

        print(f"   âœ… æ‰¾åˆ° {len(results)} æ¡ç»“æœ:")

        for i, result in enumerate(results, 1):
            content = result.get("content", "")
            metadata = result.get("metadata", {})
            content_type = metadata.get("type", "unknown")
            source = metadata.get("source", "")
            score = result.get("score", 0.0)

            print(f"      {i}. [{content_type}] {content[:40]}... ({source}, {score:.2f})")


async def demo_type_filtering(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºç±»å‹è¿‡æ»¤"""
    print("\nğŸ¯ æ­¥éª¤ 5: æ¨¡æ€è¿‡æ»¤")
    print("---")

    # åªæœç´¢å›¾åƒ
    print("\n   æœç´¢: \"ç…§ç‰‡\" + åªçœ‹å›¾åƒ")

    results = await engine.search(
        "ç…§ç‰‡",
        content_type=ContentType.IMAGE,
        top_k=5,
    )

    print(f"   âœ… æ‰¾åˆ° {len(results)} æ¡å›¾åƒç»“æœ:")

    for i, result in enumerate(results, 1):
        content = result.get("content", "")
        source = result.get("metadata", {}).get("source", "")
        print(f"      {i}. {content[:50]}... ({source})")

    # åªæœç´¢éŸ³é¢‘
    print("\n   æœç´¢: \"ä¼šè®®\" + åªå¬éŸ³é¢‘")

    results = await engine.search(
        "ä¼šè®®",
        content_type=ContentType.AUDIO,
        top_k=5,
    )

    print(f"   âœ… æ‰¾åˆ° {len(results)} æ¡éŸ³é¢‘ç»“æœ:")

    for i, result in enumerate(results, 1):
        content = result.get("content", "")
        source = result.get("metadata", {}).get("source", "")
        print(f"      {i}. {content[:50]}... ({source})")


async def demo_semantic_understanding(engine: MultimodalSearchEngine):
    """æ¼”ç¤ºè¯­ä¹‰ç†è§£"""
    print("\nğŸ’¡ æ­¥éª¤ 6: è¯­ä¹‰ç†è§£")
    print("---")

    print("\n   æµ‹è¯•è·¨æ¨¡æ€è¯­ä¹‰ç†è§£:")

    tests = [
        ("ç¾ä¸½çš„é£æ™¯", "åº”è¯¥æ‰¾åˆ°æµ·æ»©ç…§ç‰‡å’ŒåŸå¸‚å¤œæ™¯"),
        ("é‡è¦çš„è®¨è®º", "åº”è¯¥æ‰¾åˆ°ä¼šè®®éŸ³é¢‘å’Œä¼šè®®çºªè¦"),
        ("æŠ€æœ¯å­¦ä¹ ", "åº”è¯¥æ‰¾åˆ°æŠ€æœ¯æ–‡æ¡£"),
    ]

    for query, expectation in tests:
        print(f"\n   æŸ¥è¯¢: \"{query}\"")
        print(f"   æœŸæœ›: {expectation}")

        results = await engine.search(query, top_k=3)

        print(f"   ç»“æœ: æ‰¾åˆ° {len(results)} æ¡")

        if results:
            for i, result in enumerate(results[:2], 1):
                content = result.get("content", "")
                metadata = result.get("metadata", {})
                content_type = metadata.get("type", "unknown")
                print(f"      {i}. [{content_type}] {content[:40]}...")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ AgentMem å¤šæ¨¡æ€æœç´¢ç¤ºä¾‹\n")
    print("è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†:")
    print("  1. å›¾åƒæè¿°ç´¢å¼•")
    print("  2. éŸ³é¢‘è½¬å½•ç´¢å¼•")
    print("  3. æ–‡æ¡£ç´¢å¼•")
    print("  4. è·¨æ¨¡æ€æœç´¢")
    print("  5. æ¨¡æ€è¿‡æ»¤")
    print("  6. è¯­ä¹‰ç†è§£")
    print()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    api_base_url = os.getenv("AGENTMEM_API_BASE_URL", "http://localhost:8080")
    api_key = os.getenv("AGENTMEM_API_KEY", "demo_key")

    config = Config(
        api_base_url=api_base_url,
        api_key=api_key,
    )

    async with AgentMemClient(config) as client:
        print("âœ… åˆå§‹åŒ–å®Œæˆ")

        # åˆ›å»ºæœç´¢å¼•æ“
        engine = MultimodalSearchEngine(
            client=client,
            user_id="multimodal_user",
        )

        # æ¼”ç¤ºå„ç§åŠŸèƒ½
        await demo_image_indexing(engine)
        await demo_audio_indexing(engine)
        await demo_document_indexing(engine)
        await demo_cross_modal_search(engine)
        await demo_type_filtering(engine)
        await demo_semantic_understanding(engine)

        # æ˜¾ç¤ºç»Ÿè®¡
        stats = await engine.get_stats()

        print("\nğŸ“Š å†…å®¹ç»Ÿè®¡:")
        for content_type, count in sorted(stats.items()):
            print(f"   {content_type}: {count} æ¡")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        exit(1)


# ============================================
# é«˜çº§åŠŸèƒ½: å®é™…å›¾åƒå¤„ç†
# ============================================
#
# é›†æˆçœŸå®çš„å›¾åƒå¤„ç† API:
#
# ```python
# import base64
# import httpx
#
# async def describe_image(image_path: str) -> str:
#     """ä½¿ç”¨ Vision API æè¿°å›¾åƒ"""
#     # è¯»å–å›¾åƒ
#     with open(image_path, "rb") as f:
#         image_data = base64.b64encode(f.read()).decode()
#
#     # è°ƒç”¨ Vision APIï¼ˆç¤ºä¾‹ï¼‰
#     async with httpx.AsyncClient() as client:
#         response = await client.post(
#             "https://api.openai.com/v1/chat/completions",
#             headers={
#                 "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
#             },
#             json={
#                 "model": "gpt-4-vision-preview",
#                 "messages": [{
#                     "role": "user",
#                     "content": [
#                         {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
#                         {
#                             "type": "image_url",
#                             "image_url": {
#                                 "url": f"data:image/jpeg;base64,{image_data}"
#                             }
#                         }
#                     ]
#                 }]
#             }
#         )
#
#     result = response.json()
#     return result["choices"][0]["message"]["content"]
#
# # ä½¿ç”¨
# description = await describe_image("photo.jpg")
# await engine.index_content(MultimodalContent(
#     content=description,
#     content_type=ContentType.IMAGE,
#     source="photo.jpg"
# ))
# ```
#
# ============================================
# é«˜çº§åŠŸèƒ½: å®é™…éŸ³é¢‘å¤„ç†
# ============================================
#
# ```python
# async def transcribe_audio(audio_path: str) -> str:
#     """ä½¿ç”¨ Whisper API è½¬å½•éŸ³é¢‘"""
#     async with httpx.AsyncClient() as client:
#         with open(audio_path, "rb") as f:
#             response = await client.post(
#                 "https://api.openai.com/v1/audio/transcriptions",
#                 headers={
#                     "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
#                 },
#                 files={"file": f},
#                 data={"model": "whisper-1"}
#             )
#
#     result = response.json()
#     return result["text"]
# ```
#
# ============================================
# é«˜çº§åŠŸèƒ½: è§†é¢‘å¤„ç†
# ============================================
#
# ```python
# async def process_video(video_path: str) -> List[MultimodalContent]:
#     """å¤„ç†è§†é¢‘æ–‡ä»¶"""
#     contents = []
#
#     # 1. æå–å…³é”®å¸§
#     frames = extract_key_frames(video_path)
#     for i, frame in enumerate(frames):
#         description = await describe_image(frame)
#         contents.append(MultimodalContent(
#             content=f"è§†é¢‘å¸§ {i+1}: {description}",
#             content_type=ContentType.VIDEO,
#             source=video_path,
#             metadata={"frame": str(i)}
#         ))
#
#     # 2. æå–éŸ³é¢‘å¹¶è½¬å½•
#     audio_path = extract_audio(video_path)
#     transcription = await transcribe_audio(audio_path)
#     contents.append(MultimodalContent(
#         content=f"è§†é¢‘éŸ³é¢‘: {transcription}",
#         content_type=ContentType.VIDEO,
#         source=video_path,
#         metadata={"type": "audio"}
#     ))
#
#     return contents
# ```
